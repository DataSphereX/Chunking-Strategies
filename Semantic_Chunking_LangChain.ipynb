{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.llms import OpenAI  # Import LLM class\n",
    "import os\n",
    "\n",
    "# Set your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # Replace with your API key\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = OpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)  # Choose your model and parameters\n",
    "\n",
    "!wget 'https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt' -O 'pg_essay.txt'\n",
    "\n",
    "\n",
    "# Load documents (Replace with your own data loading logic)\n",
    "def load_documents():\n",
    "    loader = TextLoader(\"pg_essay.txt\")  # Path to your text file\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Perform semantic chunking\n",
    "def semantic_chunking(documents, chunk_size=512, chunk_overlap=50):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    chunks = []\n",
    "    for doc in documents:\n",
    "        chunks.extend(text_splitter.split_text(doc.page_content))\n",
    "    return chunks\n",
    "\n",
    "# Embed and store the chunks\n",
    "def embed_and_store(chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    vector_store = FAISS.from_texts(chunks, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Main RAG pipeline\n",
    "def rag_pipeline():\n",
    "    # Load and chunk documents\n",
    "    documents = load_documents()\n",
    "    chunks = semantic_chunking(documents)\n",
    "\n",
    "    # Embed and store\n",
    "    vector_store = embed_and_store(chunks)\n",
    "\n",
    "    # Set up retriever\n",
    "    retriever = vector_store.as_retriever()\n",
    "\n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0,\n",
    "        max_tokens=256\n",
    "    )\n",
    "\n",
    "    # Set up a QA chain\n",
    "    qa_chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,  # Provide the language model\n",
    "        retriever=retriever,\n",
    "        chain_type=\"stuff\",\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "    return qa_chain\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    qa_chain = rag_pipeline()\n",
    "    query = \"What is semantic chunking?\"\n",
    "    \n",
    "    # Use __call__ instead of run to get all outputs\n",
    "    result = qa_chain({\"query\": query})  # Pass the query as a dictionary\n",
    "    \n",
    "    # Extract the results\n",
    "    answer = result[\"result\"]\n",
    "    source_documents = result[\"source_documents\"]\n",
    "    \n",
    "    # Print the outputs\n",
    "    print(\"Answer:\", answer)\n",
    "    print(\"\\nSource Documents:\")\n",
    "    for doc in source_documents:\n",
    "        print(doc.page_content)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
